#cloud-config
package_update: true
package_upgrade: true

packages:
  - apt-transport-https
  - ca-certificates
  - curl
  - gnupg
  - lsb-release
  - netcat

write_files:
  - path: /var/log/cloud-init-output.log
    permissions: "0644"

runcmd:
  - echo "Starting initialization" >> /var/log/cloud-init-output.log

  # Disable swap
  - sudo swapoff -a
  - sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab

  # Enable IP forwarding
  - echo "net.ipv4.ip_forward=1" | sudo tee -a /etc/sysctl.conf
  - sudo sysctl -p

  # Add Docker repository
  - curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg
  - echo "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu $(. /etc/os-release && echo "$VERSION_CODENAME") stable" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null

  # Add Kubernetes v1.29 repository
  - curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.29/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
  - echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.29/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list

  # Update package list
  - sudo apt-get update
  - echo "Package lists updated" >> /var/log/cloud-init-output.log

  # Install Docker and Kubernetes packages
  - sudo apt-get install -y docker-ce docker-ce-cli containerd.io kubelet kubeadm kubectl
  - echo "Packages installed" >> /var/log/cloud-init-output.log

  # Hold Kubernetes packages at their installed version
  - sudo apt-mark hold kubelet kubeadm kubectl

  # Configure containerd
  - sudo mkdir -p /etc/containerd
  - containerd config default | sudo tee /etc/containerd/config.toml
  - sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml
  - sudo systemctl restart containerd
  - sudo systemctl status containerd >> /var/log/cloud-init-output.log
  - echo "Containerd configured" >> /var/log/cloud-init-output.log

  # Wait for system to settle
  - sleep 30

  # Initialize Kubernetes cluster
  - |
    max_retries=5
    retry_count=0
    until [ $retry_count -ge $max_retries ]
    do
      echo "Attempt $((retry_count+1)) of $max_retries" >> /var/log/cloud-init-output.log
      
      # Add these commands before kubeadm init
      - echo "System information:" >> /var/log/cloud-init-output.log
      - free -h >> /var/log/cloud-init-output.log
      - df -h >> /var/log/cloud-init-output.log
      - systemctl status containerd >> /var/log/cloud-init-output.log
      - systemctl status kubelet >> /var/log/cloud-init-output.log
      
      # Initialize Kubernetes cluster
      VIRTUAL_IP="10.191.182.100"
      if sudo kubeadm init \
        --control-plane-endpoint "${VIRTUAL_IP}:6443" \
        --upload-certs \
        --pod-network-cidr=192.168.0.0/16 \
        --v=5 >> /var/log/cloud-init-output.log 2>&1; then
        echo "Kubernetes initialization successful" >> /var/log/cloud-init-output.log
        break
      fi
      
      retry_count=$((retry_count+1))
      echo "Initialization failed, waiting before retry..." >> /var/log/cloud-init-output.log
      sleep 30
    done

    if [ $retry_count -ge $max_retries ]; then
      echo "Failed to initialize Kubernetes after $max_retries attempts" >> /var/log/cloud-init-output.log
      exit 1
    fi

  - echo "Kubernetes initialized" >> /var/log/cloud-init-output.log
  # Set up kubeconfig for the ubuntu user
  - mkdir -p $HOME/.kube
  - sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  - sudo chown $(id -u):$(id -g) $HOME/.kube/config
  - echo "Kubeconfig set up for ubuntu user" >> /var/log/cloud-init-output.log

  # Ensure the correct permissions and ownership
  - sudo chmod 644 /etc/kubernetes/admin.conf
  - sudo chown root:root /etc/kubernetes/admin.conf

  # Wait for API server to be ready
  - timeout 900 bash -c 'until kubectl get nodes &>/dev/null; do sleep 10; echo "Waiting for API server..." >> /var/log/cloud-init-output.log; done'
  - echo "API server is ready" >> /var/log/cloud-init-output.log

  # Install Calico
  - kubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.26.1/manifests/tigera-operator.yaml
  - kubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.26.1/manifests/custom-resources.yaml
  - echo "Calico installed" >> /var/log/cloud-init-output.log

  # Generate join command for other nodes
  - kubeadm token create --print-join-command > /home/ubuntu/join-command.sh
  - chmod +x /home/ubuntu/join-command.sh

  # Make join command accessible to other nodes
  - nohup bash -c 'while true; do echo "$(cat /home/ubuntu/join-command.sh)" | nc -l -p 8000; done' &
  - echo "Join command server started" >> /var/log/cloud-init-output.log

  # Ensure kube-proxy is running
  - kubectl get pods -n kube-system | grep kube-proxy
  - echo "Kube-proxy status checked" >> /var/log/cloud-init-output.log

  # Configure kube-proxy mode (if needed)
  - |
    kubectl -n kube-system get configmap kube-proxy -o yaml > kube-proxy-config.yaml
    sed -i 's/mode: ""/mode: "ipvs"/' kube-proxy-config.yaml
    kubectl apply -f kube-proxy-config.yaml
    rm kube-proxy-config.yaml
  - echo "Kube-proxy configured" >> /var/log/cloud-init-output.log
  # Create sample deployment and service
  - kubectl create deployment nginx --image=nginx
  - kubectl expose deployment nginx --port=80 --type=NodePort

  # Create scenarios for troubleshooting
  - kubectl taint nodes control-plane-1 key1=value1:NoSchedule
  - kubectl create deployment nginx-taint --image=nginx --replicas=3

  # Set up monitoring
  - kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml

  # Additional CKA exam scenarios
  - kubectl create configmap my-config --from-literal=key1=value1
  - kubectl create secret generic my-secret --from-literal=username=admin --from-literal=password=secret
  - kubectl apply -f https://k8s.io/examples/pods/storage/pv-volume.yaml
  - kubectl apply -f https://k8s.io/examples/pods/storage/pv-claim.yaml

  # Additional troubleshooting steps
  - echo "Checking API server status" >> /var/log/cloud-init-output.log
  - sudo crictl pods | grep kube-apiserver >> /var/log/cloud-init-output.log
  - echo "Checking kubelet status" >> /var/log/cloud-init-output.log
  - sudo systemctl status kubelet >> /var/log/cloud-init-output.log
  - echo "Checking API server logs" >> /var/log/cloud-init-output.log
  - sudo crictl logs $(sudo crictl pods | grep kube-apiserver | awk '{print $1}') >> /var/log/cloud-init-output.log
  - echo "Checking kubeconfig" >> /var/log/cloud-init-output.log
  - cat $HOME/.kube/config >> /var/log/cloud-init-output.log

  # Install and configure CoreDNS
  - |
    cat <<EOF | kubectl apply -f -
    apiVersion: v1
    kind: ConfigMap
    metadata:
      name: coredns-custom
      namespace: kube-system
    data:
      Corefile: |
        .:53 {
            errors
            health {
                lameduck 5s
            }
            ready
            kubernetes cluster.local in-addr.arpa ip6.arpa {
                pods insecure
                fallthrough in-addr.arpa ip6.arpa
                ttl 30
            }
            prometheus :9153
            forward . /etc/resolv.conf {
                max_concurrent 1000
            }
            cache 30
            loop
            reload
            loadbalance
        }
    EOF
  - echo "CoreDNS configured" >> /var/log/cloud-init-output.log
  # Configure network policies
  - |
    cat <<EOF | kubectl apply -f -
    apiVersion: networking.k8s.io/v1
    kind: NetworkPolicy
    metadata:
      name: default-deny-ingress
      namespace: default
    spec:
      podSelector: {}
      policyTypes:
      - Ingress
    EOF
  - echo "Default network policy applied" >> /var/log/cloud-init-output.log
  # Configure metrics server with proper settings
  - |
    cat <<EOF | kubectl apply -f -
    apiVersion: v1
    kind: ServiceAccount
    metadata:
      labels:
        k8s-app: metrics-server
      name: metrics-server
      namespace: kube-system
    ---
    apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: metrics-server
      namespace: kube-system
      labels:
        k8s-app: metrics-server
    spec:
      selector:
        matchLabels:
          k8s-app: metrics-server
      template:
        metadata:
          labels:
            k8s-app: metrics-server
        spec:
          containers:
          - args:
            - --cert-dir=/tmp
            - --secure-port=4443
            - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
            - --kubelet-use-node-status-port
            - --metric-resolution=15s
            - --kubelet-insecure-tls
            image: registry.k8s.io/metrics-server/metrics-server:v0.6.4
            name: metrics-server
            ports:
            - containerPort: 4443
              name: https
              protocol: TCP
          priorityClassName: system-cluster-critical
    EOF
  - echo "Metrics server configured" >> /var/log/cloud-init-output.log
  # Verify critical components
  - kubectl wait --for=condition=Ready pods --all -n kube-system --timeout=300s
  - echo "All system pods are ready" >> /var/log/cloud-init-output.log

  # Configure resource quotas for namespaces
  - |
    cat <<EOF | kubectl apply -f -
    apiVersion: v1
    kind: ResourceQuota
    metadata:
      name: compute-resources
      namespace: default
    spec:
      hard:
        requests.cpu: "4"
        requests.memory: 4Gi
        limits.cpu: "8"
        limits.memory: 8Gi
    EOF
  - echo "Resource quota configured" >> /var/log/cloud-init-output.log
  # Configure Pod Security Standards
  - |
    cat <<EOF | kubectl apply -f -
    apiVersion: v1
    kind: Namespace
    metadata:
      name: restricted
      labels:
        pod-security.kubernetes.io/enforce: restricted
        pod-security.kubernetes.io/audit: restricted
        pod-security.kubernetes.io/warn: restricted
    EOF
  - echo "Pod Security Standards configured" >> /var/log/cloud-init-output.log
  # Configure etcd backup
  - |
    cat <<EOF > /etc/kubernetes/etcd-backup.sh
    #!/bin/bash
    ETCDCTL_API=3 etcdctl --endpoints=https://127.0.0.1:2379 \
      --cacert=/etc/kubernetes/pki/etcd/ca.crt \
      --cert=/etc/kubernetes/pki/etcd/server.crt \
      --key=/etc/kubernetes/pki/etcd/server.key \
      snapshot save /var/lib/etcd/backup/etcd-snapshot-\$(date +%Y%m%d).db
    EOF
  - chmod +x /etc/kubernetes/etcd-backup.sh
  - mkdir -p /var/lib/etcd/backup
  - echo "0 */6 * * * root /etc/kubernetes/etcd-backup.sh" > /etc/cron.d/etcd-backup
  - echo "etcd backup configured" >> /var/log/cloud-init-output.log

  # Configure audit logging
  - |
    cat <<EOF | sudo tee /etc/kubernetes/audit-policy.yaml
    apiVersion: audit.k8s.io/v1
    kind: Policy
    rules:
    - level: Metadata
      resources:
      - group: ""
        resources: ["pods", "services", "configmaps"]
    - level: RequestResponse
      resources:
      - group: ""
        resources: ["secrets"]
    EOF
  - mkdir -p /var/log/kubernetes/audit
  - echo "Audit logging configured" >> /var/log/cloud-init-output.log

  # Update API server configuration to enable audit logging
  - |
    sudo sed -i '/--enable-admission-plugins=/ s/$/,PodSecurityPolicy/' /etc/kubernetes/manifests/kube-apiserver.yaml
    sudo sed -i '/- kube-apiserver/a\    - --audit-policy-file=/etc/kubernetes/audit-policy.yaml\n    - --audit-log-path=/var/log/kubernetes/audit/audit.log' /etc/kubernetes/manifests/kube-apiserver.yaml
  - echo "API server configuration updated" >> /var/log/cloud-init-output.log
  # Configure log rotation for audit logs
  - |
    cat <<EOF | sudo tee /etc/logrotate.d/kubernetes-audit
    /var/log/kubernetes/audit/audit.log {
        rotate 5
        copytruncate
        missingok
        notifempty
        compress
        maxsize 100M
        daily
        create 0644 root root
    }
    EOF
  - echo "Log rotation configured" >> /var/log/cloud-init-output.log
  - echo "Setup complete" >> /var/log/cloud-init-output.log

  # Configure inter-namespace network policies
  - |
    cat <<EOF | kubectl apply -f -
    apiVersion: networking.k8s.io/v1
    kind: NetworkPolicy
    metadata:
      name: allow-selected-namespaces
      namespace: restricted
    spec:
      podSelector: {}
      policyTypes:
      - Ingress
      ingress:
      - from:
        - namespaceSelector:
            matchLabels:
              access: allowed
    EOF
  - echo "Inter-namespace network policy configured" >> /var/log/cloud-init-output.log
  # Label default namespace for network policy
  - kubectl label namespace default access=allowed
  - echo "Default namespace labeled" >> /var/log/cloud-init-output.log

  # Install Prometheus and Grafana for monitoring
  - |
    cat <<EOF | kubectl apply -f -
    apiVersion: v1
    kind: Namespace
    metadata:
      name: monitoring
    ---
    apiVersion: source.toolkit.fluxcd.io/v1beta2
    kind: HelmRepository
    metadata:
      name: prometheus-community
      namespace: monitoring
    spec:
      interval: 1h
      url: https://prometheus-community.github.io/helm-charts
    ---
    apiVersion: helm.toolkit.fluxcd.io/v2beta1
    kind: HelmRelease
    metadata:
      name: kube-prometheus-stack
      namespace: monitoring
    spec:
      interval: 5m
      chart:
        spec:
          chart: kube-prometheus-stack
          version: ">=45.x"
          sourceRef:
            kind: HelmRepository
            name: prometheus-community
          interval: 1m
      values:
        grafana:
          adminPassword: admin
        prometheus:
          prometheusSpec:
            retention: 5d
            resources:
              requests:
                cpu: 200m
                memory: 200Mi
            storageSpec:
              volumeClaimTemplate:
                spec:
                  accessModes: ["ReadWriteOnce"]
                  resources:
                    requests:
                      storage: 10Gi
    EOF
  - echo "Prometheus and Grafana installed" >> /var/log/cloud-init-output.log
  # Configure high availability for etcd
  - |
    cat <<EOF | kubectl apply -f -
    apiVersion: v1
    kind: ConfigMap
    metadata:
      name: etcd-config
      namespace: kube-system
    data:
      etcd.conf: |
        ETCD_HEARTBEAT_INTERVAL=100
        ETCD_ELECTION_TIMEOUT=1000
        ETCD_MAX_SNAPSHOTS=5
        ETCD_MAX_WALS=5
        ETCD_QUOTA_BACKEND_BYTES=8589934592
    EOF
  - echo "etcd HA configuration applied" >> /var/log/cloud-init-output.log
  # Add node affinity rules for critical components
  - |
    cat <<EOF | kubectl apply -f -
    apiVersion: v1
    kind: ConfigMap
    metadata:
      name: scheduler-config
      namespace: kube-system
    data:
      config.yaml: |
        apiVersion: kubescheduler.config.k8s.io/v1
        kind: KubeSchedulerConfiguration
        profiles:
        - schedulerName: default-scheduler
          plugins:
            score:
              enabled:
              - name: NodeResourcesBalancedAllocation
                weight: 2
              - name: NodePreferAvoidPods
                weight: 10000
    EOF
  - echo "Scheduler configuration updated" >> /var/log/cloud-init-output.log
  # Add cluster autoscaling configuration (if needed)
  - |
    cat <<EOF | kubectl apply -f -
    apiVersion: v1
    kind: ServiceAccount
    metadata:
      name: cluster-autoscaler
      namespace: kube-system
    ---
    apiVersion: rbac.authorization.k8s.io/v1
    kind: ClusterRole
    metadata:
      name: cluster-autoscaler
    rules:
      - apiGroups: [""]
        resources: ["nodes"]
        verbs: ["watch", "list", "get"]
      - apiGroups: [""]
        resources: ["pods"]
        verbs: ["watch", "list", "get"]
    EOF
  - echo "Cluster autoscaling configured" >> /var/log/cloud-init-output.log
  # Final verification of all components
  - kubectl get nodes -o wide
  - kubectl get pods --all-namespaces
  - echo "Final verification completed" >> /var/log/cloud-init-output.log
